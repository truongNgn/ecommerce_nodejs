# ‚öñÔ∏è CHI·∫æN L∆Ø·ª¢C M·ªû R·ªòNG (SCALING STRATEGY)

**T√†i li·ªáu**: Ph√¢n t√≠ch chi·∫øn l∆∞·ª£c m·ªü r·ªông h·ªá th·ªëng  
**Ng√†y**: 28/10/2025  
**T√°c gi·∫£**: Team T10_N12

---

## üìã M·ª§C L·ª§C

1. [T·ªïng quan](#1-t·ªïng-quan)
2. [Ph√¢n t√≠ch replicas](#2-ph√¢n-t√≠ch-replicas)
3. [Load distribution](#3-load-distribution)
4. [Resource allocation](#4-resource-allocation)
5. [Scaling scenarios](#5-scaling-scenarios)

---

## 1. T·ªîNG QUAN

### 1.1. Current Replica Configuration

| Service | Replicas | L√Ω do |
|---------|----------|-------|
| **Backend** | 3 | API server - high traffic, stateless |
| **Frontend** | 2 | Static serving - medium load, stateless |
| **Worker** | 2 | Background jobs - parallel processing |
| **Nginx** | 1 | Load balancer - single entry point |
| **MongoDB** | 1 | Stateful database - single source of truth |
| **Redis** | 1 | In-memory cache - single instance ƒë·ªß |
| **Visualizer** | 1 | Monitoring UI - low priority |
| **TOTAL** | **12** | Total replicas across cluster |

### 1.2. Scaling Philosophy

```
          HIGH TRAFFIC                 MEDIUM LOAD              LOW LOAD
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Backend √ó 3       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Frontend √ó 2    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Others √ó 1    ‚îÇ
    ‚îÇ   Worker √ó 2        ‚îÇ     ‚îÇ                  ‚îÇ    ‚îÇ  (Stateful)    ‚îÇ
    ‚îÇ   (Stateless)       ‚îÇ     ‚îÇ  (Stateless)     ‚îÇ    ‚îÇ                ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         SCALE OUT                   SCALE OUT            NO SCALING
    (Horizontal scaling)        (Horizontal scaling)   (Vertical only)
```

**Nguy√™n t·∫Øc**:
- **Stateless services**: Scale horizontally (th√™m replicas)
- **Stateful services**: Scale vertically (tƒÉng resources) ho·∫∑c d√πng clustering
- **Critical services**: More replicas (backend, worker)
- **Supporting services**: Fewer replicas (frontend)

---

## 2. PH√ÇN T√çCH REPLICAS

### 2.1. Backend - 3 Replicas

**L√Ω do ch·ªçn 3**:
1. ‚úÖ **High availability**: N·∫øu 1 replica down ‚Üí c√≤n 2 serving traffic
2. ‚úÖ **Load balancing**: Ph√¢n t√°n requests qua 3 instances
3. ‚úÖ **Zero downtime**: Rolling update 1 replica/l·∫ßn, c√≤n 2 serving
4. ‚úÖ **Odd number**: Tr√°nh split-brain trong consensus algorithms (future-proof)

**Traffic pattern**:
```
            Nginx Load Balancer
                    ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ           ‚îÇ           ‚îÇ
        ‚ñº           ‚ñº           ‚ñº
   Backend-1    Backend-2    Backend-3
    (33.3%)      (33.3%)      (33.3%)
```

**Failure scenarios**:
- **1 replica down**: 2 c√≤n l·∫°i handle 50% each (acceptable)
- **2 replicas down**: 1 c√≤n l·∫°i handle 100% (degraded but working)
- **3 replicas down**: Service unavailable (requires manual intervention)

**Capacity planning**:
- Each replica: 500 req/min (theoretical)
- 3 replicas: 1500 req/min total
- Peak traffic: ~800 req/min (53% utilization)
- Room for growth: +87% capacity

### 2.2. Worker - 2 Replicas

**L√Ω do ch·ªçn 2**:
1. ‚úÖ **Parallel processing**: 2 workers process queue simultaneously
2. ‚úÖ **Redundancy**: 1 worker down ‚Üí c√≤n 1 processing
3. ‚úÖ **Queue distribution**: Bull queue auto-distribute jobs
4. ‚úÖ **Cost effective**: Email sending kh√¥ng c·∫ßn qu√° nhi·ªÅu workers

**Job processing pattern**:
```
         Redis Queue (Bull)
                ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ               ‚îÇ
        ‚ñº               ‚ñº
    Worker-1        Worker-2
   (Process A)     (Process B)
        ‚îÇ               ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚ñº
         Email sent ‚úÖ
```

**Processing capacity**:
- Each worker: ~120 emails/hour (avg 30s per email)
- 2 workers: ~240 emails/hour
- Current volume: ~50-80 emails/hour (33% utilization)

**Job types**:
- Order confirmation emails (high priority)
- Welcome emails (medium priority)
- Password reset emails (high priority)
- Promotional emails (low priority)

### 2.3. Frontend - 2 Replicas

**L√Ω do ch·ªçn 2**:
1. ‚úÖ **Redundancy**: 1 replica down ‚Üí c√≤n 1 serving
2. ‚úÖ **Static content**: √çt resource-intensive h∆°n backend
3. ‚úÖ **CDN alternative**: Poor man's CDN v·ªõi 2 instances
4. ‚úÖ **Update safety**: Rolling update an to√†n

**Traffic pattern**:
```
            Nginx Load Balancer
                    ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ               ‚îÇ
            ‚ñº               ‚ñº
       Frontend-1      Frontend-2
        (50%)           (50%)
            ‚îÇ               ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚ñº
            Static assets
         (HTML, CSS, JS, images)
```

**Why not 3+ replicas?**:
- Static content ‚Üí low compute cost
- Nginx already caches static assets
- 2 replicas ƒë·ªß cho current traffic
- Ti·∫øt ki·ªám resources cho backend/worker

### 2.4. Nginx - 1 Replica

**L√Ω do ch·ªâ 1**:
1. ‚úÖ **Single entry point**: Nginx l√† front door, kh√¥ng c·∫ßn multiple doors
2. ‚úÖ **Lightweight**: Nginx r·∫•t efficient, 1 instance handle 10k+ connections
3. ‚úÖ **Port 80 constraint**: Only 1 service bind port 80 (ingress mode)
4. ‚úÖ **Swarm routing mesh**: Swarm auto-route ƒë·∫øn replica kh·∫£ d·ª•ng

**High availability**:
```
   External traffic (port 80)
            ‚îÇ
            ‚ñº
    Swarm Routing Mesh
            ‚îÇ
            ‚ñº
        Nginx (1 replica)
            ‚îÇ
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ         ‚îÇ         ‚îÇ
  ‚ñº         ‚ñº         ‚ñº
Backend  Frontend  Other
(3x)      (2x)    (1x each)
```

**N·∫øu Nginx down**:
- Swarm auto-restart trong 5-10s
- Frontend/Backend services v·∫´n accessible via direct ports (dev mode)
- Production n√™n d√πng external load balancer (AWS ALB, GCP Load Balancer)

### 2.5. MongoDB & Redis - 1 Replica

**L√Ω do ch·ªâ 1**:
1. ‚úÖ **Stateful services**: Kh√¥ng th·ªÉ scale ngang ƒë∆°n gi·∫£n
2. ‚úÖ **Data consistency**: Single source of truth tr√°nh conflicts
3. ‚úÖ **Volume persistence**: Data l∆∞u trong volumes, survive restarts
4. ‚úÖ **Current load**: Sufficient for current traffic

**Future scaling options**:

**MongoDB**:
```
   Single instance (current)
            ‚îÇ
            ‚ñº
   MongoDB Replica Set (future)
            ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ       ‚îÇ       ‚îÇ
  Primary Secondary Secondary
    (R/W)   (R)     (R)
```

**Redis**:
```
   Single instance (current)
            ‚îÇ
            ‚ñº
   Redis Sentinel (future)
            ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ       ‚îÇ       ‚îÇ
  Master  Replica Sentinel
   (R/W)    (R)    (Monitor)
```

---

## 3. LOAD DISTRIBUTION

### 3.1. Request Flow

```mermaid
sequenceDiagram
    participant U as User
    participant N as Nginx
    participant B1 as Backend-1
    participant B2 as Backend-2
    participant B3 as Backend-3
    participant M as MongoDB

    U->>N: HTTP Request
    Note over N: Load balancing<br/>algorithm: least_conn
    
    alt Backend-1 has fewer connections
        N->>B1: Forward request
        B1->>M: Query database
        M-->>B1: Data
        B1-->>N: Response
    else Backend-2 has fewer connections
        N->>B2: Forward request
        B2->>M: Query database
        M-->>B2: Data
        B2-->>N: Response
    else Backend-3 has fewer connections
        N->>B3: Forward request
        B3->>M: Query database
        M-->>B3: Data
        B3-->>N: Response
    end
    
    N-->>U: Response
```

### 3.2. Load Balancing Algorithm

**Nginx s·ª≠ d·ª•ng `least_conn`**:
```nginx
upstream backend {
    least_conn;  # Ch·ªçn backend v·ªõi √≠t connections nh·∫•t
    server backend:5000 max_fails=3 fail_timeout=30s;
}
```

**So s√°nh algorithms**:

| Algorithm | Description | Use case |
|-----------|-------------|----------|
| **round_robin** | Xoay v√≤ng ƒë·ªÅu | Requests t∆∞∆°ng t·ª± nhau |
| **least_conn** | √çt connections nh·∫•t | ‚úÖ Requests kh√°c nhau v·ªÅ processing time |
| **ip_hash** | D·ª±a v√†o client IP | Session affinity (sticky sessions) |
| **hash** | D·ª±a v√†o custom key | Custom logic |

**T·∫°i sao ch·ªçn `least_conn`?**:
- ‚úÖ Requests kh√°c nhau (GET /products fast, POST /orders slow)
- ‚úÖ Tr√°nh overload 1 backend
- ‚úÖ Fair distribution based on actual load

### 3.3. Connection Distribution

**Example scenario**:
```
Initial state:
Backend-1: 0 connections
Backend-2: 0 connections
Backend-3: 0 connections

After 10 requests:
Backend-1: 3 connections (2 long-running)
Backend-2: 4 connections (all short)
Backend-3: 3 connections (1 long-running)

Next request ‚Üí Backend-2 (most available)
```

---

## 4. RESOURCE ALLOCATION

### 4.1. Total Resources

**CPU allocation**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Total CPU: ~3.6 cores                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Backend (3x):   1.5 cores (41.7%)              ‚îÇ
‚îÇ Worker (2x):    1.0 cores (27.8%)              ‚îÇ
‚îÇ Frontend (2x):  0.5 cores (13.9%)              ‚îÇ
‚îÇ MongoDB:        0.5 cores (13.9%)              ‚îÇ
‚îÇ Others:         0.1 cores (2.8%)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Memory allocation**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Total Memory: ~3.5 GB                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Backend (3x):   1.5 GB (42.9%)                 ‚îÇ
‚îÇ MongoDB:        1.0 GB (28.6%)                 ‚îÇ
‚îÇ Worker (2x):    1.0 GB (28.6%)                 ‚îÇ
‚îÇ Frontend (2x):  0.5 GB (14.3%)                 ‚îÇ
‚îÇ Redis:          0.3 GB (8.6%)                  ‚îÇ
‚îÇ Others:         0.2 GB (5.7%)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4.2. Resource Optimization

**Per-replica resources**:

| Service | CPU Limit | Memory Limit | Replicas | Total CPU | Total Memory |
|---------|-----------|--------------|----------|-----------|--------------|
| Backend | 0.5 | 512M | 3 | 1.5 | 1536M |
| Worker | 0.5 | 512M | 2 | 1.0 | 1024M |
| Frontend | 0.25 | 256M | 2 | 0.5 | 512M |
| MongoDB | 1.0 | 1G | 1 | 1.0 | 1024M |
| Redis | 0.25 | 256M | 1 | 0.25 | 256M |
| Nginx | 0.25 | 128M | 1 | 0.25 | 128M |
| Visualizer | 0.1 | 64M | 1 | 0.1 | 64M |
| **TOTAL** | | | **12** | **3.6** | **4544M (~4.4G)** |

**Host requirements**:
- Minimum: 4 CPU cores, 6GB RAM
- Recommended: 8 CPU cores, 12GB RAM (50% buffer)
- Production: 16 CPU cores, 24GB RAM (100% buffer)

---

## 5. SCALING SCENARIOS

### 5.1. Traffic Patterns

**Daily pattern**:
```
 Requests/min
     ‚îÇ
1500 ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     ‚îÇ                    ‚ï±‚ï≤
1000 ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï±  ‚ï≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     ‚îÇ              ‚ï±‚îÄ‚îÄ‚îÄ‚ï±    ‚ï≤‚îÄ‚îÄ‚îÄ‚ï≤
 500 ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï±‚îÄ‚îÄ‚îÄ‚ï±              ‚ï≤‚îÄ‚îÄ‚îÄ‚ï≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     ‚îÇ    ‚ï±‚îÄ‚îÄ‚îÄ‚ï±                        ‚ï≤‚îÄ‚îÄ‚îÄ‚ï≤‚îÄ
   0 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     0  3  6  9  12 15 18 21 24  (hours)
     
     Morning   Peak      Evening   Night
     (low)    (high)     (medium)  (low)
```

**Peak hours**: 10:00 - 14:00, 18:00 - 21:00  
**Low hours**: 00:00 - 06:00

### 5.2. Scaling Actions

**Scenario 1: Traffic tƒÉng 2x**
```bash
# Scale backend
docker service scale ecommerce_backend=5

# Scale worker (if email queue grows)
docker service scale ecommerce_worker=3

# Scale frontend (if static requests increase)
docker service scale ecommerce_frontend=3
```

**New distribution**:
- Backend: 3 ‚Üí 5 (capacity +67%)
- Worker: 2 ‚Üí 3 (capacity +50%)
- Frontend: 2 ‚Üí 3 (capacity +50%)

**Scenario 2: Black Friday / Sales event**
```bash
# Pre-scale before event
docker service scale ecommerce_backend=7
docker service scale ecommerce_worker=4
docker service scale ecommerce_frontend=4

# Monitor and adjust
docker service logs ecommerce_backend --follow

# Scale down after event
docker service scale ecommerce_backend=3
docker service scale ecommerce_worker=2
docker service scale ecommerce_frontend=2
```

**Scenario 3: MongoDB reaching limits**
```bash
# Option 1: Vertical scaling (increase resources)
docker service update \
  --limit-cpu 2 \
  --limit-memory 2G \
  ecommerce_mongo

# Option 2: Migrate to MongoDB Replica Set
# (Requires architecture change)
```

### 5.3. Auto-scaling (Future)

**Docker Swarm kh√¥ng h·ªó tr·ª£ auto-scaling native**, nh∆∞ng c√≥ th·ªÉ implement:

**Option 1: Custom script**
```bash
#!/bin/bash
# auto-scale.sh

# Get current CPU usage
CPU=$(docker stats --no-stream --format "{{.CPUPerc}}" ecommerce_backend | cut -d'%' -f1)

if (( $(echo "$CPU > 80" | bc -l) )); then
  echo "High CPU, scaling up..."
  CURRENT=$(docker service inspect -f '{{.Spec.Mode.Replicated.Replicas}}' ecommerce_backend)
  NEW=$((CURRENT + 1))
  docker service scale ecommerce_backend=$NEW
fi
```

**Option 2: Kubernetes (migration)**
```yaml
# HorizontalPodAutoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: backend-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: backend
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

### 5.4. Scaling Metrics

**Monitor these metrics**:
```bash
# CPU usage
docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}"

# Response time
curl -w "@curl-format.txt" -o /dev/null -s http://localhost:8080/api/health

# Request rate
docker service logs ecommerce_nginx | grep -c "200 OK"

# Queue length
docker exec $(docker ps -q -f name=ecommerce_redis) redis-cli LLEN emailQueue
```

**Thresholds**:
- CPU > 70%: Consider scaling
- Response time > 200ms: Scale immediately
- Queue length > 100: Scale workers
- Error rate > 1%: Investigate before scaling

---

## 6. K·∫æT LU·∫¨N

### 6.1. Scaling Strategy Summary

‚úÖ **Backend 3 replicas**: High availability, load distribution  
‚úÖ **Worker 2 replicas**: Parallel processing, redundancy  
‚úÖ **Frontend 2 replicas**: Redundancy, cost-effective  
‚úÖ **Nginx 1 replica**: Single entry point, efficient  
‚úÖ **Stateful 1 replica**: Data consistency  

### 6.2. Key Decisions

1. **Prioritize backend scaling**: API server l√† bottleneck ch√≠nh
2. **Worker parallelism**: Email queue c·∫ßn parallel processing
3. **Frontend minimal**: Static content kh√¥ng c·∫ßn nhi·ªÅu replicas
4. **Stateful vertical**: MongoDB/Redis scale vertically ho·∫∑c clustering
5. **Nginx single**: Load balancer kh√¥ng ph·∫£i bottleneck

### 6.3. Future Improvements

1. **Implement monitoring**: Prometheus + Grafana
2. **Auto-scaling**: Custom scripts ho·∫∑c migrate Kubernetes
3. **Database clustering**: MongoDB Replica Set, Redis Sentinel
4. **Cache layer**: Redis cache cho frequent queries
5. **CDN**: Cloudflare/AWS CloudFront cho static assets

---

**Chi·∫øn l∆∞·ª£c m·ªü r·ªông ƒë∆∞·ª£c thi·∫øt k·∫ø d·ª±a tr√™n traffic patterns, resource constraints, v√† high availability requirements.**

**Ng∆∞·ªùi t·∫°o**: Team T10_N12  
**Ng√†y**: 28/10/2025
